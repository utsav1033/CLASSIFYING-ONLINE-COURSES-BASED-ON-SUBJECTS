# CLASSIFYING-ONLINE-COURSES-BASED-ON-SUBJECTS

Course: NLP (Semester 6) - Pillai College of Engineering
Project Overview
This project is part of the Natural Language Processing (NLP) course for Semester 6 students at Pillai College of Engineering. The objective is to classify online courses into predefined categories such as Technical, Business, Creative, and Humanities. The classification is performed using Machine Learning (ML), Deep Learning (DL), and Transformer-based Language Models.

The project involves text preprocessing, feature extraction, model training, and performance evaluation to determine the most effective approach for course classification. By implementing different classification techniques, we aim to improve course discoverability and personalized recommendations for students.

Acknowledgements
We would like to express our sincere gratitude to the following individuals for their guidance and support throughout this project:

Theory Faculty:

Dhiraj Amin

Sharvari Govilkar

Lab Faculty:

Dhiraj Amin

Neha Ashok

Shubhangi Chavan

Project Title
Classifying Online Courses Based on Subjects Using Natural Language Processing

Project Abstract
The project focuses on automatically categorizing online courses into different subject areas to facilitate better organization and recommendation on e-learning platforms. The classification is performed using Machine Learning, Deep Learning, and Transformer-based models, such as BERT and RoBERTa.

By applying techniques such as text vectorization, feature engineering, and model optimization, we aim to improve classification accuracy and efficiency. The comparative analysis of traditional ML models, deep learning architectures, and language models provides insights into their effectiveness in handling course classification tasks.

Algorithms Used
Machine Learning Algorithms
Logistic Regression

Support Vector Machine (SVM)

Random Forest Classifier

Deep Learning Algorithms
Convolutional Neural Networks (CNN)

Recurrent Neural Networks (RNN)

Long Short-Term Memory (LSTM)

Language Models
BERT (Bidirectional Encoder Representations from Transformers)

RoBERTa (Robustly Optimized BERT Approach)

Comparative Analysis
The performance of different models is evaluated based on accuracy, precision, recall, and F1-score. The table below summarizes the results:

Model Type	Accuracy (%)	Precision (%)	Recall (%)	F1-Score (%)
Logistic Regression	80.5	79.8	81.0	80.4
SVM (Support Vector Machine)	83.2	82.5	84.1	83.3
Random Forest	86.7	85.9	87.5	86.7
CNN (Convolutional Neural Networks)	89.4	88.2	90.0	89.1
RNN (Recurrent Neural Networks)	87.9	86.7	88.5	87.6
LSTM (Long Short-Term Memory)	91.1	90.3	92.0	91.1
BERT (Bidirectional Encoder Representations from Transformers)	94.2	93.6	95.0	94.3
RoBERTa (Robustly Optimized BERT Approach)	95.0	94.4	95.8	95.1
Conclusion
This project highlights the effectiveness of ML, DL, and Transformer-based models in classifying online courses into subject-based categories. The results demonstrate that BERT and RoBERTa outperform traditional ML and deep learning models, achieving the highest accuracy and reliability in course classification.

By leveraging state-of-the-art NLP models, this project contributes to the development of better course recommendation systems for e-learning platforms, enabling efficient course discovery and personalized learning experiences.
